{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM0xi8Gmm+5jxZ9GiXZjTT6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomdeepAcharyya/NLP/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6zm1gY9q30b"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import shutil\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "from nltk.tokenize import word_tokenize, blankline_tokenize  # tokenization\n",
        "from nltk.probability import FreqDist    # frequency of words \n",
        "from nltk.util import bigrams, trigrams, ngrams     # dividing sentence into phrases\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer   # stemming\n",
        "from nltk.stem import wordnet, WordNetLemmatizer    # lemmatization\n",
        "from nltk import ne_chunk   # Named Entity Recognition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjctcblDsU7u",
        "outputId": "f18d60c1-097e-467b-8aff-8a4033ffff2d"
      },
      "source": [
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "brown.words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yal9CsHztioF"
      },
      "source": [
        "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
        "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpiZBgyS3mKp"
      },
      "source": [
        "ai = 'Less than a decade after breaking the Nazi encryption machine Enigma and helping the Allied Forces win World War II, mathematician Alan Turing changed history a second time with a simple question: Can machines think? '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbsEZVHt3_Xo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5728bb7b-00cf-46b8-c22b-f1cce4d4a3d4"
      },
      "source": [
        "nltk.download('punkt')\n",
        "ai_tokens = word_tokenize(ai)\n",
        "ai_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Less',\n",
              " 'than',\n",
              " 'a',\n",
              " 'decade',\n",
              " 'after',\n",
              " 'breaking',\n",
              " 'the',\n",
              " 'Nazi',\n",
              " 'encryption',\n",
              " 'machine',\n",
              " 'Enigma',\n",
              " 'and',\n",
              " 'helping',\n",
              " 'the',\n",
              " 'Allied',\n",
              " 'Forces',\n",
              " 'win',\n",
              " 'World',\n",
              " 'War',\n",
              " 'II',\n",
              " ',',\n",
              " 'mathematician',\n",
              " 'Alan',\n",
              " 'Turing',\n",
              " 'changed',\n",
              " 'history',\n",
              " 'a',\n",
              " 'second',\n",
              " 'time',\n",
              " 'with',\n",
              " 'a',\n",
              " 'simple',\n",
              " 'question',\n",
              " ':',\n",
              " 'Can',\n",
              " 'machines',\n",
              " 'think',\n",
              " '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgTSqcbS4JjO"
      },
      "source": [
        "fdist =  FreqDist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIHC5mnX5Fz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142f65d3-8eb5-46ce-ecb5-9a87593b667f"
      },
      "source": [
        "for word in ai_tokens:\n",
        "  fdist[word.lower()] += 1\n",
        "fdist"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({',': 1,\n",
              "          ':': 1,\n",
              "          '?': 1,\n",
              "          'a': 3,\n",
              "          'after': 1,\n",
              "          'alan': 1,\n",
              "          'allied': 1,\n",
              "          'and': 1,\n",
              "          'breaking': 1,\n",
              "          'can': 1,\n",
              "          'changed': 1,\n",
              "          'decade': 1,\n",
              "          'encryption': 1,\n",
              "          'enigma': 1,\n",
              "          'forces': 1,\n",
              "          'helping': 1,\n",
              "          'history': 1,\n",
              "          'ii': 1,\n",
              "          'less': 1,\n",
              "          'machine': 1,\n",
              "          'machines': 1,\n",
              "          'mathematician': 1,\n",
              "          'nazi': 1,\n",
              "          'question': 1,\n",
              "          'second': 1,\n",
              "          'simple': 1,\n",
              "          'than': 1,\n",
              "          'the': 2,\n",
              "          'think': 1,\n",
              "          'time': 1,\n",
              "          'turing': 1,\n",
              "          'war': 1,\n",
              "          'win': 1,\n",
              "          'with': 1,\n",
              "          'world': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc3AIMtS5cUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "178f933b-2276-4890-900a-e517d2d25f0a"
      },
      "source": [
        "ai_blank = blankline_tokenize(ai)\n",
        "ai_blank"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Less than a decade after breaking the Nazi encryption machine Enigma and helping the Allied Forces win World War II, mathematician Alan Turing changed history a second time with a simple question: Can machines think? ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af7Ti6M26Npu"
      },
      "source": [
        "ai_ngrams = list(nltk.ngrams(ai_tokens, 7))\n",
        "ai_ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJPJfeGw7EQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af10a728-abc5-4c22-a1a8-fb2192a711c1"
      },
      "source": [
        "pst = PorterStemmer()\n",
        "lst = LancasterStemmer()\n",
        "snb = SnowballStemmer('english')\n",
        "stemm_words = []\n",
        "for words in ai_tokens:\n",
        "  stemm_words.append(snb.stem(words))\n",
        "  print(words+':'+snb.stem(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Less:less\n",
            "than:than\n",
            "a:a\n",
            "decade:decad\n",
            "after:after\n",
            "breaking:break\n",
            "the:the\n",
            "Nazi:nazi\n",
            "encryption:encrypt\n",
            "machine:machin\n",
            "Enigma:enigma\n",
            "and:and\n",
            "helping:help\n",
            "the:the\n",
            "Allied:alli\n",
            "Forces:forc\n",
            "win:win\n",
            "World:world\n",
            "War:war\n",
            "II:ii\n",
            ",:,\n",
            "mathematician:mathematician\n",
            "Alan:alan\n",
            "Turing:ture\n",
            "changed:chang\n",
            "history:histori\n",
            "a:a\n",
            "second:second\n",
            "time:time\n",
            "with:with\n",
            "a:a\n",
            "simple:simpl\n",
            "question:question\n",
            ":::\n",
            "Can:can\n",
            "machines:machin\n",
            "think:think\n",
            "?:?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYoB4zc49hHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a6fdd9-e0d5-4682-f37f-1acf290e0478"
      },
      "source": [
        "word_len = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "lemm_words = []\n",
        "for words in ai_tokens:\n",
        "  lemm_words.append(word_len.lemmatize(words))\n",
        "  print(words+':'+word_len.lemmatize(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Less:Less\n",
            "than:than\n",
            "a:a\n",
            "decade:decade\n",
            "after:after\n",
            "breaking:breaking\n",
            "the:the\n",
            "Nazi:Nazi\n",
            "encryption:encryption\n",
            "machine:machine\n",
            "Enigma:Enigma\n",
            "and:and\n",
            "helping:helping\n",
            "the:the\n",
            "Allied:Allied\n",
            "Forces:Forces\n",
            "win:win\n",
            "World:World\n",
            "War:War\n",
            "II:II\n",
            ",:,\n",
            "mathematician:mathematician\n",
            "Alan:Alan\n",
            "Turing:Turing\n",
            "changed:changed\n",
            "history:history\n",
            "a:a\n",
            "second:second\n",
            "time:time\n",
            "with:with\n",
            "a:a\n",
            "simple:simple\n",
            "question:question\n",
            ":::\n",
            "Can:Can\n",
            "machines:machine\n",
            "think:think\n",
            "?:?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbwYF4I1_kiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa8cc9d1-bbb6-4e2d-cb07-d0e31faee2e2"
      },
      "source": [
        "punc_words = []\n",
        "punctuation = re.compile(r'[-.?!,:;()|0-9]')\n",
        "for words in lemm_words:\n",
        "  word =  punctuation.sub(\"\", words)\n",
        "  if len(word) > 0:\n",
        "    punc_words.append(word)\n",
        "punc_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Less',\n",
              " 'than',\n",
              " 'a',\n",
              " 'decade',\n",
              " 'after',\n",
              " 'breaking',\n",
              " 'the',\n",
              " 'Nazi',\n",
              " 'encryption',\n",
              " 'machine',\n",
              " 'Enigma',\n",
              " 'and',\n",
              " 'helping',\n",
              " 'the',\n",
              " 'Allied',\n",
              " 'Forces',\n",
              " 'win',\n",
              " 'World',\n",
              " 'War',\n",
              " 'II',\n",
              " 'mathematician',\n",
              " 'Alan',\n",
              " 'Turing',\n",
              " 'changed',\n",
              " 'history',\n",
              " 'a',\n",
              " 'second',\n",
              " 'time',\n",
              " 'with',\n",
              " 'a',\n",
              " 'simple',\n",
              " 'question',\n",
              " 'Can',\n",
              " 'machine',\n",
              " 'think']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o66WE-9FBlSV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd4d261-45e7-42f1-d627-3ce8172857b4"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "ai_pos = nltk.pos_tag(punc_words)\n",
        "ai_pos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Less', 'JJR'),\n",
              " ('than', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('decade', 'NN'),\n",
              " ('after', 'IN'),\n",
              " ('breaking', 'VBG'),\n",
              " ('the', 'DT'),\n",
              " ('Nazi', 'JJ'),\n",
              " ('encryption', 'NN'),\n",
              " ('machine', 'NN'),\n",
              " ('Enigma', 'NNP'),\n",
              " ('and', 'CC'),\n",
              " ('helping', 'VBG'),\n",
              " ('the', 'DT'),\n",
              " ('Allied', 'NNP'),\n",
              " ('Forces', 'NNPS'),\n",
              " ('win', 'VBP'),\n",
              " ('World', 'NNP'),\n",
              " ('War', 'NNP'),\n",
              " ('II', 'NNP'),\n",
              " ('mathematician', 'NN'),\n",
              " ('Alan', 'NNP'),\n",
              " ('Turing', 'NNP'),\n",
              " ('changed', 'VBD'),\n",
              " ('history', 'NN'),\n",
              " ('a', 'DT'),\n",
              " ('second', 'JJ'),\n",
              " ('time', 'NN'),\n",
              " ('with', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('simple', 'JJ'),\n",
              " ('question', 'NN'),\n",
              " ('Can', 'MD'),\n",
              " ('machine', 'NN'),\n",
              " ('think', 'VB')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X9nM4NSCsvV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "44a82313-1d39-4d28-c7e5-49b00b0b1b6b"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "ai_ner = ne_chunk(punc_words)\n",
        "ai_ner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-aba9ca05fb32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maxent_ne_chunker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mai_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunc_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mai_ner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/chunk/__init__.py\u001b[0m in \u001b[0;36mne_chunk\u001b[0;34m(tagged_tokens, binary)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mne_chunk_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/chunk/named_entity.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mEach\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtagged\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tagged_to_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mtag_one\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtagger\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_taggers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mchoose_tag\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# Use our feature detector to get the featureset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mfeatureset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# Use the classifier to pick a tag.  If a cutoff probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tag/sequential.py\u001b[0m in \u001b[0;36mfeature_detector\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/chunk/named_entity.py\u001b[0m in \u001b[0;36m_feature_detector\u001b[0;34m(self, tokens, index, history)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mnextpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mnextnextword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mnextnextpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# 89.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: string index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XJTmCdorUGa"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtNuVNfrvPEo"
      },
      "source": [
        "def get_user_score():\n",
        "  import spacy\n",
        "  spc = spacy.load('en_core_web_sm')\n",
        "  keywords = 'bengal west election campaign news bjp trinamool chief minister communists india hindu muslim mla mp vidhan sabha bengali bangla tmc cpim cpm mamata modi'\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QDFScoTMGZh"
      },
      "source": [
        "# Importing TextBlob\n",
        "from textblob import TextBlob\n",
        "# Creating a textblob object and assigning the sentiment propertygete\n",
        "def get_polarity(a):\n",
        "  analysis = TextBlob(a).polarity\n",
        "  return analysis\n",
        "def get_subjectivity(a):\n",
        "  analysis = TextBlob(a).subjectivity\n",
        "  return analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzDU2d3kMKEB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}